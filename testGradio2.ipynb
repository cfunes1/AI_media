{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68322dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#beyond the interface class, working with blocks\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    name = gr.Textbox(label=\"Name\")\n",
    "    output = gr.Textbox(label=\"Output Box\")\n",
    "    greet_btn = gr.Button(\"Greet\")\n",
    "    greet_btn.click(fn=greet, inputs=name, outputs=output, api_name=\"greet\")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8856d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# event listeners using decorators\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    name = gr.Textbox(label=\"Name\")\n",
    "    output = gr.Textbox(label=\"Output Box\")\n",
    "    greet_btn = gr.Button(\"Greet\")\n",
    "\n",
    "    @greet_btn.click(inputs=name, outputs=output)\n",
    "    def greet(name):\n",
    "        return \"Hello \" + name + \"!\"\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0375381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def welcome(name):\n",
    "    return f\"Welcome to Gradio, {name}!\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Hello World!\n",
    "    Start typing below to see the output.\n",
    "    \"\"\")\n",
    "    inp = gr.Textbox(placeholder=\"What is your name?\")\n",
    "    out = gr.Textbox()\n",
    "    inp.change(welcome, inp, out)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fcbb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def increase(num):\n",
    "    return num + 1\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    a = gr.Number(label=\"a\")\n",
    "    b = gr.Number(label=\"b\")\n",
    "    atob = gr.Button(\"a > b\")\n",
    "    btoa = gr.Button(\"b > a\")\n",
    "    atob.click(increase, a, b)\n",
    "    btoa.click(increase, b, a)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5584d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "asr = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")\n",
    "classifier = pipeline(\"text-classification\")\n",
    "\n",
    "def speech_to_text(speech):\n",
    "    text = asr(speech)[\"text\"]  \n",
    "    return text\n",
    "\n",
    "def text_to_sentiment(text):\n",
    "    return classifier(text)[0][\"label\"]  \n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    audio_file = gr.Audio(type=\"filepath\")\n",
    "    text = gr.Textbox()\n",
    "    label = gr.Label()\n",
    "\n",
    "    b1 = gr.Button(\"Recognize Speech\")\n",
    "    b2 = gr.Button(\"Classify Sentiment\")\n",
    "\n",
    "    b1.click(speech_to_text, inputs=audio_file, outputs=text)\n",
    "    b2.click(text_to_sentiment, inputs=text, outputs=label)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1269b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple inputs components\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    a = gr.Number(label=\"a\")\n",
    "    b = gr.Number(label=\"b\")\n",
    "    with gr.Row():\n",
    "        add_btn = gr.Button(\"Add\")\n",
    "        sub_btn = gr.Button(\"Subtract\")\n",
    "    c = gr.Number(label=\"sum\")\n",
    "\n",
    "    def add(num1, num2):\n",
    "        return num1 + num2\n",
    "    add_btn.click(add, inputs=[a, b], outputs=c)\n",
    "\n",
    "    def sub(data):\n",
    "        return data[a] - data[b]\n",
    "    sub_btn.click(sub, inputs={a, b}, outputs=c)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80673ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e9aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Carlos\\Documents\\Code\\AI_media\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Users\\Carlos\\Documents\\Code\\AI_media\\env\\Lib\\site-packages\\ctranslate2\\__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.12.2)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared.\n",
      "Running faster whisper model locally. \n",
      "file_path='C:\\\\Users\\\\cfune\\\\AppData\\\\Local\\\\Temp\\\\gradio\\\\bdf4155afd7fc989b61f424f8b2c36a309dc949201c4bfed0de733954a537a9c\\\\1_Audio.mp3'\n",
      " model_size='distil-large-v3'\n",
      " device='cuda'\n",
      " compute_type='float16'\n",
      " language=None\n",
      " prompt=None\n",
      "\n",
      "Detected language en with probability 0.85693359375\n",
      "GPU cache cleared.\n",
      "Running whisper model locally. \n",
      "file_path='C:\\\\Users\\\\cfune\\\\AppData\\\\Local\\\\Temp\\\\gradio\\\\bdf4155afd7fc989b61f424f8b2c36a309dc949201c4bfed0de733954a537a9c\\\\1_Audio.mp3'\n",
      " model_size='large-v3'\n",
      " device='cuda'\n",
      " verbose=True\n",
      " prompt=None\n",
      " language=None\n",
      "\n",
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: Spanish\n",
      "[00:00.000 --> 00:05.580]  Es bien sabido que grupos criminales amasan grandes fortunas que inciden en las econom√≠as de los pa√≠ses.\n",
      "[00:05.900 --> 00:14.220]  Para combatirla y evitar el lavado de capitales, un grupo de expertos ha brindado su experiencia en el libro titulado Econom√≠as Criminales.\n",
      "[00:14.280 --> 00:23.060]  La econom√≠a criminal es un negocio, es un negocio multimillonario que va desde el tr√°fico de drogas hasta el tr√°fico de personas.\n",
      "[00:23.060 --> 00:31.880]  Y por supuesto tiene tambi√©n dentro del negocio el comercio il√≠cito, el contrabando de alcohol, hidrocarburos, tabaco, etc.\n",
      "[00:32.440 --> 00:39.440]  Seg√∫n los expertos, este fen√≥meno criminal no tiene fronteras e inicia principalmente en la corrupci√≥n de funcionarios p√∫blicos,\n",
      "[00:39.840 --> 00:44.900]  provocando y dando puerta a la criminalidad y por supuesto dejando una estela de muerte.\n",
      "[00:45.380 --> 00:50.880]  Toda la regi√≥n se encuentra erosionada por la corrupci√≥n.\n",
      "[00:51.760 --> 00:52.900]  Y por lo tanto esto gira.\n",
      "[00:53.060 --> 01:00.920]  Genera que el movimiento ilegal de las econom√≠as ilegales se mueva de pa√≠s en pa√≠s.\n",
      "[01:02.120 --> 01:07.960]  Hoy d√≠a particularmente hay un inter√©s significativo en los puertos.\n",
      "[01:08.380 --> 01:14.960]  Y eso hace que tanto los puertos del Pac√≠fico como del Atl√°ntico sean objetivos del crimen organizado.\n",
      "[01:16.100 --> 01:21.460]  Por eso tambi√©n en los puertos es donde se produce el mayor n√∫mero de muertes,\n",
      "[01:21.960 --> 01:22.960]  donde el sicariato termina.\n",
      "[01:23.060 --> 01:24.880]  Y es donde se obtiene la mayor importancia.\n",
      "[01:24.920 --> 01:30.480]  La econom√≠a criminal se caracteriza por su versatilidad y flexibilidad en su organizaci√≥n,\n",
      "[01:31.020 --> 01:38.100]  las que permiten eludir las regulaciones nacionales y los procedimientos de colaboraci√≥n policial internacional.\n",
      "[01:38.100 --> 01:45.060]  Es una empresa que tiene operadores log√≠sticos, abogados, contadores, etc.\n",
      "[01:45.220 --> 01:51.460]  y que compite por debajo de la mesa y por los caminos verdes justamente con la empresa y el negocio legal.\n",
      "[01:51.760 --> 01:52.900]  El libro tambi√©n detalla...\n",
      "[01:53.060 --> 01:55.060]  por medio de sus autores...\n",
      "[01:55.060 --> 01:58.740]  cu√°les son las nuevas f√≥rmulas para combatir este flagelo.\n",
      "[01:58.740 --> 02:06.060]  Tener en cuenta que las autoridades necesitan m√∫sculo de inteligencia y m√∫sculo de investigaci√≥n criminal\n",
      "[02:06.060 --> 02:09.660]  con el prop√≥sito de fortalecer la lucha contra la corrupci√≥n.\n",
      "[02:09.660 --> 02:13.060]  Donde hay corrupci√≥n, donde hay delito, hay corrupci√≥n.\n",
      "[02:13.060 --> 02:16.060]  La corrupci√≥n es el principal combustible de las econom√≠as criminales.\n",
      "[02:16.060 --> 02:23.020]  Tambi√©n para desarticular las estructuras criminales, las cabezas, estas mafias que est√°n detr√°s de este sistema de conducta.\n",
      "[02:23.060 --> 02:25.060]  La corrupci√≥n es un gran problema para el pa√≠s.\n",
      "[02:25.060 --> 02:27.060]  La corrupci√≥n es un gran problema para la econom√≠a criminal.\n",
      "[02:27.060 --> 02:29.060]  Tercero, para perseguir el dinero que es la principal motivaci√≥n de estas econom√≠as criminales.\n",
      "[02:29.060 --> 02:35.060]  Y cuarto, en el prop√≥sito de quitarles los bienes con procesos de extinci√≥n de dominio que les permite...\n",
      "[02:35.060 --> 02:39.060]  Nico Medefrias, AN Panam√°, Noticias de Panam√° para el Mundo.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "import numpy as np\n",
    "from carlos_tools_audio import OpenAI_transcribe, local_whisper_transcribe, local_faster_whisper_transcribe\n",
    "from carlos_tools_misc import clear_GPU_cache\n",
    "import tempfile\n",
    "import soundfile as sf\n",
    "\n",
    "# Import your local whisper and faster-whisper models\n",
    "from transformers import pipeline\n",
    "import faster_whisper\n",
    "\n",
    "# Dummy GPT remote transcription function (replace with your actual API call)\n",
    "def gpt_transcribe(path):\n",
    "    response = OpenAI_transcribe(\n",
    "        path,\n",
    "        model=\"whisper-1\",\n",
    "        response_format=\"text\"\n",
    "    )\n",
    "    text=response[\"text\"]\n",
    "    duration=response[\"inference_time\"]\n",
    "    return text, duration\n",
    "\n",
    "# Local Whisper\n",
    "def whisper_transcribe(path):\n",
    "    clear_GPU_cache()\n",
    "    response = local_whisper_transcribe(\n",
    "        path,\n",
    "        model_size=\"large-v3\",\n",
    "    )\n",
    "    text= response[\"text\"]\n",
    "    duration= response[\"inference_time\"]\n",
    "    return text, duration\n",
    "\n",
    "# Faster Whisper\n",
    "def faster_whisper_transcribe(path):\n",
    "    clear_GPU_cache()\n",
    "    response = local_faster_whisper_transcribe(\n",
    "        path,\n",
    "        model_size=\"distil-large-v3\",\n",
    "    )\n",
    "    text = response[\"text\"]\n",
    "    duration = response[\"inference_time\"]\n",
    "    return text, duration\n",
    "\n",
    "def compare_transcriptions(path):\n",
    "    gpt_text, gpt_time = gpt_transcribe(path)\n",
    "    faster_text, faster_time = faster_whisper_transcribe(path)\n",
    "    whisper_text, whisper_time = whisper_transcribe(path)\n",
    "    table = [\n",
    "        # [\"Model\", \"Transcription\", \"Duration (s)\"],\n",
    "        [\"GPT (remote)\", gpt_text, round(gpt_time, 2)],\n",
    "        [\"Whisper (local)\", whisper_text, round(whisper_time, 2)],\n",
    "        [\"Faster Whisper (local)\", faster_text, round(faster_time, 2)],\n",
    "    ]\n",
    "    return table\n",
    "\n",
    "with gr.Blocks() as demo:   # may tray themes e.g. theme=gr.themes.Soft()\n",
    "    with gr.Sidebar(position=\"left\", width=200, visible=True):\n",
    "        gr.Markdown(\"# Carlos' tests\")\n",
    "        gr.Markdown(\"## Audio\")\n",
    "        gr.Markdown(\"### [Speech-To-Text](https://huggingface.co/openai/whisper-large-v3)\")\n",
    "\n",
    "    gr.Markdown(\"\"\"\n",
    "            # Speech-To-Text tasks\n",
    "            \"\"\")\n",
    "    with gr.Tab(\"Transcribe\"):\n",
    "            with gr.Row():\n",
    "                gr.Markdown(\"\"\"\n",
    "                            # TRANSCRIPTION\n",
    "                            ## compare transcriptions from different models\n",
    "                            ### Models used: \n",
    "                            \"\"\")\n",
    "            with gr.Row():\n",
    "                gr.Markdown(\"\"\"\n",
    "                            ### [Whisper](https://huggingface.co/openai/whisper-large-v3)\n",
    "\n",
    "                            Whisper is  essentially a language model grounded in audio ‚Äî an audio-conditional GPT. It was trained by OpenAI on a large and diverse dataset of multilingual audio, enabling it to perform automatic speech recognition (ASR) and translation tasks across many languages.\n",
    "\n",
    "                            Whisper is trained in a similar fashion to the original GPT, using self-supervised learning with a next-token prediction objective. However, while GPT is trained solely on text, Whisper is trained on paired audio and text, where the model learns to generate transcriptions (or translations) token by token from audio inputs. During training, the encoder processes the audio into latent representations, and the decoder learns to predict the next text token given the previous tokens and the audio context. Unlike GPT, which relies purely on textual continuity, Whisper must also learn alignment between speech and language, making it a multimodal model trained end-to-end on large-scale audio-text datasets.\n",
    "                            \"\"\")\n",
    "                gr.Markdown(\"\"\"\n",
    "                            ### [Faster Whisper](https://huggingface.co/Systran/faster-whisper-large-v3)\n",
    "\n",
    "                            Faster-Whisper is a high-performance, inference-optimized implementation of OpenAI's Whisper model ‚Äî effectively a language model grounded in audio, engineered for fast, resource-efficient deployment.\n",
    "\n",
    "                            While it retains the same underlying architecture and training paradigm as Whisper ‚Äî a multimodal encoder-decoder transformer trained via self-supervised next-token prediction on paired audio-text data ‚Äî Faster-Whisper focuses entirely on inference-time efficiency. It uses CTranslate2, a highly optimized inference engine for transformer models, to significantly accelerate transcription and translation while reducing memory usage.\n",
    "\n",
    "                            Like Whisper, Faster-Whisper takes in raw audio, encodes it into latent representations via the audio encoder, and then decodes text token by token, conditioned on both the encoded audio and previously generated tokens. However, all training is inherited directly from the original Whisper checkpoints ‚Äî Faster-Whisper is not retrained, but instead recompiled and optimized for speed and portability (e.g., on CPU, GPU, or ARM devices).\n",
    "\n",
    "                            As a result, Faster-Whisper makes Whisper‚Äôs powerful multilingual speech recognition capabilities more accessible in production environments, edge devices, and real-time applications where latency and efficiency are critical.\n",
    "                            \"\"\")\n",
    "\n",
    "            audio_input = gr.Audio(sources=\"upload\", type= \"filepath\", label=\"Upload Audio\")\n",
    "            output_table = gr.Dataframe(\n",
    "                headers=[\"Model\", \"Translation\", \"Duration (s)\"],\n",
    "                datatype=[\"str\", \"str\", \"number\"],  # Ensure \"Translation\" is string\n",
    "                row_count=5,  # Adjust as needed for visible rows\n",
    "                interactive=False\n",
    ")\n",
    "            transcribe_btn = gr.Button(\"Transcribe with All Models\")\n",
    "            transcribe_btn.click(compare_transcriptions, inputs=audio_input, outputs=output_table)\n",
    "    with gr.Tab(\"Translate\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "                        # Speech-To-Text tasks: TRANSLATION\n",
    "                        ## compare translations from different models\n",
    "                        ### Models used: \n",
    "                        ### [Whisper](https://huggingface.co/openai/whisper-large-v3)\n",
    "\n",
    "                        Whisper is  essentially a language model grounded in audio ‚Äî an audio-conditional GPT. It was trained by OpenAI on a large and diverse dataset of multilingual audio, enabling it to perform automatic speech recognition (ASR) and translation tasks across many languages.\n",
    "\n",
    "                        Whisper is trained in a similar fashion to the original GPT, using self-supervised learning with a next-token prediction objective. However, while GPT is trained solely on text, Whisper is trained on paired audio and text, where the model learns to generate transcriptions (or translations) token by token from audio inputs. During training, the encoder processes the audio into latent representations, and the decoder learns to predict the next text token given the previous tokens and the audio context. Unlike GPT, which relies purely on textual continuity, Whisper must also learn alignment between speech and language, making it a multimodal model trained end-to-end on large-scale audio-text datasets.\n",
    "\n",
    "                        ### [Faster Whisper](https://huggingface.co/Systran/faster-whisper-large-v3)\n",
    "\n",
    "                        Faster-Whisper is a high-performance, inference-optimized implementation of OpenAI's Whisper model ‚Äî effectively a language model grounded in audio, engineered for fast, resource-efficient deployment.\n",
    "\n",
    "                        While it retains the same underlying architecture and training paradigm as Whisper ‚Äî a multimodal encoder-decoder transformer trained via self-supervised next-token prediction on paired audio-text data ‚Äî Faster-Whisper focuses entirely on inference-time efficiency. It uses CTranslate2, a highly optimized inference engine for transformer models, to significantly accelerate transcription and translation while reducing memory usage.\n",
    "\n",
    "                        Like Whisper, Faster-Whisper takes in raw audio, encodes it into latent representations via the audio encoder, and then decodes text token by token, conditioned on both the encoded audio and previously generated tokens. However, all training is inherited directly from the original Whisper checkpoints ‚Äî Faster-Whisper is not retrained, but instead recompiled and optimized for speed and portability (e.g., on CPU, GPU, or ARM devices).\n",
    "\n",
    "                        As a result, Faster-Whisper makes Whisper‚Äôs powerful multilingual speech recognition capabilities more accessible in production environments, edge devices, and real-time applications where latency and efficiency are critical.\n",
    "                        \"\"\")\n",
    "            audio_input = gr.Audio(sources=\"upload\", type= \"filepath\", label=\"Upload Audio\")\n",
    "            # output_table = gr.Dataframe(headers=[\"Model\", \"Translation\", \"Duration (s)\"], interactive=False)\n",
    "            output_table = gr.Dataframe(\n",
    "                headers=[\"Model\", \"Translation\", \"Duration (s)\"],\n",
    "                datatype=[\"str\", \"str\", \"number\"],  # Ensure \"Translation\" is string\n",
    "                row_count=5,  # Adjust as needed for visible rows\n",
    "                interactive=False\n",
    ")\n",
    "            transcribe_btn = gr.Button(\"Translate with All Models\")\n",
    "            transcribe_btn.click(compare_transcriptions, inputs=audio_input, outputs=output_table)\n",
    "    with gr.Tab(\"Detect language\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "                        # Speech-To-Text tasks: LANGUAGE DETECTION\n",
    "                        ## compare language detection from different models\n",
    "                        ### Models used: \n",
    "                        ### [Whisper](https://huggingface.co/openai/whisper-large-v3)\n",
    "\n",
    "                        Whisper is  essentially a language model grounded in audio ‚Äî an audio-conditional GPT. It was trained by OpenAI on a large and diverse dataset of multilingual audio, enabling it to perform automatic speech recognition (ASR) and translation tasks across many languages.\n",
    "\n",
    "                        Whisper is trained in a similar fashion to the original GPT, using self-supervised learning with a next-token prediction objective. However, while GPT is trained solely on text, Whisper is trained on paired audio and text, where the model learns to generate transcriptions (or translations) token by token from audio inputs. During training, the encoder processes the audio into latent representations, and the decoder learns to predict the next text token given the previous tokens and the audio context. Unlike GPT, which relies purely on textual continuity, Whisper must also learn alignment between speech and language, making it a multimodal model trained end-to-end on large-scale audio-text datasets.\n",
    "\n",
    "                        ### [Faster Whisper](https://huggingface.co/Systran/faster-whisper-large-v3)\n",
    "\n",
    "                        Faster-Whisper is a high-performance, inference-optimized implementation of OpenAI's Whisper model ‚Äî effectively a language model grounded in audio, engineered for fast, resource-efficient deployment.\n",
    "\n",
    "                        While it retains the same underlying architecture and training paradigm as Whisper ‚Äî a multimodal encoder-decoder transformer trained via self-supervised next-token prediction on paired audio-text data ‚Äî Faster-Whisper focuses entirely on inference-time efficiency. It uses CTranslate2, a highly optimized inference engine for transformer models, to significantly accelerate transcription and translation while reducing memory usage.\n",
    "\n",
    "                        Like Whisper, Faster-Whisper takes in raw audio, encodes it into latent representations via the audio encoder, and then decodes text token by token, conditioned on both the encoded audio and previously generated tokens. However, all training is inherited directly from the original Whisper checkpoints ‚Äî Faster-Whisper is not retrained, but instead recompiled and optimized for speed and portability (e.g., on CPU, GPU, or ARM devices).\n",
    "\n",
    "                        As a result, Faster-Whisper makes Whisper‚Äôs powerful multilingual speech recognition capabilities more accessible in production environments, edge devices, and real-time applications where latency and efficiency are critical.\n",
    "                        \"\"\")\n",
    "            audio_input = gr.Audio(sources=\"upload\", type= \"filepath\", label=\"Upload Audio\")\n",
    "            output_table = gr.Dataframe(headers=[\"Model\", \"Language\", \"Duration (s)\"], interactive=False)\n",
    "            transcribe_btn = gr.Button(\"Detect Language with All Models\")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b50800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whisper in HF\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-large-v3-turbo\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0\n",
    ")\n",
    "pipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78dfc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "\n",
    "def generate_pet_name(animal_type, personality):\n",
    "    cute_prefixes = [\"Fluffy\", \"Ziggy\", \"Bubbles\", \"Pickle\", \"Waffle\", \"Mochi\", \"Cookie\", \"Pepper\"]\n",
    "    animal_suffixes = {\n",
    "        \"Cat\": [\"Whiskers\", \"Paws\", \"Mittens\", \"Purrington\"],\n",
    "        \"Dog\": [\"Woofles\", \"Barkington\", \"Waggins\", \"Pawsome\"],\n",
    "        \"Bird\": [\"Feathers\", \"Wings\", \"Chirpy\", \"Tweets\"],\n",
    "        \"Rabbit\": [\"Hops\", \"Cottontail\", \"Bouncy\", \"Fluff\"]\n",
    "    }\n",
    "\n",
    "    prefix = random.choice(cute_prefixes)\n",
    "    suffix = random.choice(animal_suffixes[animal_type])\n",
    "\n",
    "    if personality == \"Silly\":\n",
    "        prefix = random.choice([\"Sir\", \"Lady\", \"Captain\", \"Professor\"]) + \" \" + prefix\n",
    "    elif personality == \"Royal\":\n",
    "        suffix += \" the \" + random.choice([\"Great\", \"Magnificent\", \"Wise\", \"Brave\"])\n",
    "\n",
    "    return f\"{prefix} {suffix}\"\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    with gr.Sidebar(position=\"left\"):\n",
    "        gr.Markdown(\"# üêæ Pet Name Generator\")\n",
    "        gr.Markdown(\"Use the options below to generate a unique pet name!\")\n",
    "\n",
    "        animal_type = gr.Dropdown(\n",
    "            choices=[\"Cat\", \"Dog\", \"Bird\", \"Rabbit\"],\n",
    "            label=\"Choose your pet type\",\n",
    "            value=\"Cat\"\n",
    "        )\n",
    "        personality = gr.Radio(\n",
    "            choices=[\"Normal\", \"Silly\", \"Royal\"],\n",
    "            label=\"Personality type\",\n",
    "            value=\"Normal\"\n",
    "        )\n",
    "\n",
    "    name_output = gr.Textbox(label=\"Your pet's fancy name:\", lines=2)\n",
    "    generate_btn = gr.Button(\"Generate Name! üé≤\", variant=\"primary\")\n",
    "    generate_btn.click(\n",
    "        fn=generate_pet_name,\n",
    "        inputs=[animal_type, personality],\n",
    "        outputs=name_output\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be19c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_textbox = gr.Textbox()\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n",
    "    input_textbox.render()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac33cfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared.\n"
     ]
    }
   ],
   "source": [
    "from carlos_tools_misc import clear_GPU_cache\n",
    "clear_GPU_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
