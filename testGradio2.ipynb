{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68322dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#beyond the interface class, working with blocks\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    name = gr.Textbox(label=\"Name\")\n",
    "    output = gr.Textbox(label=\"Output Box\")\n",
    "    greet_btn = gr.Button(\"Greet\")\n",
    "    greet_btn.click(fn=greet, inputs=name, outputs=output, api_name=\"greet\")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8856d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# event listeners using decorators\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    name = gr.Textbox(label=\"Name\")\n",
    "    output = gr.Textbox(label=\"Output Box\")\n",
    "    greet_btn = gr.Button(\"Greet\")\n",
    "\n",
    "    @greet_btn.click(inputs=name, outputs=output)\n",
    "    def greet(name):\n",
    "        return \"Hello \" + name + \"!\"\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0375381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def welcome(name):\n",
    "    return f\"Welcome to Gradio, {name}!\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Hello World!\n",
    "    Start typing below to see the output.\n",
    "    \"\"\")\n",
    "    inp = gr.Textbox(placeholder=\"What is your name?\")\n",
    "    out = gr.Textbox()\n",
    "    inp.change(welcome, inp, out)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fcbb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def increase(num):\n",
    "    return num + 1\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    a = gr.Number(label=\"a\")\n",
    "    b = gr.Number(label=\"b\")\n",
    "    atob = gr.Button(\"a > b\")\n",
    "    btoa = gr.Button(\"b > a\")\n",
    "    atob.click(increase, a, b)\n",
    "    btoa.click(increase, b, a)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5584d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "asr = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")\n",
    "classifier = pipeline(\"text-classification\")\n",
    "\n",
    "def speech_to_text(speech):\n",
    "    text = asr(speech)[\"text\"]  \n",
    "    return text\n",
    "\n",
    "def text_to_sentiment(text):\n",
    "    return classifier(text)[0][\"label\"]  \n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    audio_file = gr.Audio(type=\"filepath\")\n",
    "    text = gr.Textbox()\n",
    "    label = gr.Label()\n",
    "\n",
    "    b1 = gr.Button(\"Recognize Speech\")\n",
    "    b2 = gr.Button(\"Classify Sentiment\")\n",
    "\n",
    "    b1.click(speech_to_text, inputs=audio_file, outputs=text)\n",
    "    b2.click(text_to_sentiment, inputs=text, outputs=label)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1269b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple inputs components\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    a = gr.Number(label=\"a\")\n",
    "    b = gr.Number(label=\"b\")\n",
    "    with gr.Row():\n",
    "        add_btn = gr.Button(\"Add\")\n",
    "        sub_btn = gr.Button(\"Subtract\")\n",
    "    c = gr.Number(label=\"sum\")\n",
    "\n",
    "    def add(num1, num2):\n",
    "        return num1 + num2\n",
    "    add_btn.click(add, inputs=[a, b], outputs=c)\n",
    "\n",
    "    def sub(data):\n",
    "        return data[a] - data[b]\n",
    "    sub_btn.click(sub, inputs={a, b}, outputs=c)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80673ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e9aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Carlos\\Documents\\Code\\AI_media\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Users\\Carlos\\Documents\\Code\\AI_media\\env\\Lib\\site-packages\\ctranslate2\\__init__.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.12.2)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared.\n",
      "Running faster whisper model locally. \n",
      "file_path='C:\\\\Users\\\\cfune\\\\AppData\\\\Local\\\\Temp\\\\gradio\\\\bdf4155afd7fc989b61f424f8b2c36a309dc949201c4bfed0de733954a537a9c\\\\1_Audio.mp3'\n",
      " model_size='distil-large-v3'\n",
      " device='cuda'\n",
      " compute_type='float16'\n",
      " language=None\n",
      " prompt=None\n",
      "\n",
      "Detected language en with probability 0.85693359375\n",
      "GPU cache cleared.\n",
      "Running whisper model locally. \n",
      "file_path='C:\\\\Users\\\\cfune\\\\AppData\\\\Local\\\\Temp\\\\gradio\\\\bdf4155afd7fc989b61f424f8b2c36a309dc949201c4bfed0de733954a537a9c\\\\1_Audio.mp3'\n",
      " model_size='large-v3'\n",
      " device='cuda'\n",
      " verbose=True\n",
      " prompt=None\n",
      " language=None\n",
      "\n",
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: Spanish\n",
      "[00:00.000 --> 00:05.580]  Es bien sabido que grupos criminales amasan grandes fortunas que inciden en las economías de los países.\n",
      "[00:05.900 --> 00:14.220]  Para combatirla y evitar el lavado de capitales, un grupo de expertos ha brindado su experiencia en el libro titulado Economías Criminales.\n",
      "[00:14.280 --> 00:23.060]  La economía criminal es un negocio, es un negocio multimillonario que va desde el tráfico de drogas hasta el tráfico de personas.\n",
      "[00:23.060 --> 00:31.880]  Y por supuesto tiene también dentro del negocio el comercio ilícito, el contrabando de alcohol, hidrocarburos, tabaco, etc.\n",
      "[00:32.440 --> 00:39.440]  Según los expertos, este fenómeno criminal no tiene fronteras e inicia principalmente en la corrupción de funcionarios públicos,\n",
      "[00:39.840 --> 00:44.900]  provocando y dando puerta a la criminalidad y por supuesto dejando una estela de muerte.\n",
      "[00:45.380 --> 00:50.880]  Toda la región se encuentra erosionada por la corrupción.\n",
      "[00:51.760 --> 00:52.900]  Y por lo tanto esto gira.\n",
      "[00:53.060 --> 01:00.920]  Genera que el movimiento ilegal de las economías ilegales se mueva de país en país.\n",
      "[01:02.120 --> 01:07.960]  Hoy día particularmente hay un interés significativo en los puertos.\n",
      "[01:08.380 --> 01:14.960]  Y eso hace que tanto los puertos del Pacífico como del Atlántico sean objetivos del crimen organizado.\n",
      "[01:16.100 --> 01:21.460]  Por eso también en los puertos es donde se produce el mayor número de muertes,\n",
      "[01:21.960 --> 01:22.960]  donde el sicariato termina.\n",
      "[01:23.060 --> 01:24.880]  Y es donde se obtiene la mayor importancia.\n",
      "[01:24.920 --> 01:30.480]  La economía criminal se caracteriza por su versatilidad y flexibilidad en su organización,\n",
      "[01:31.020 --> 01:38.100]  las que permiten eludir las regulaciones nacionales y los procedimientos de colaboración policial internacional.\n",
      "[01:38.100 --> 01:45.060]  Es una empresa que tiene operadores logísticos, abogados, contadores, etc.\n",
      "[01:45.220 --> 01:51.460]  y que compite por debajo de la mesa y por los caminos verdes justamente con la empresa y el negocio legal.\n",
      "[01:51.760 --> 01:52.900]  El libro también detalla...\n",
      "[01:53.060 --> 01:55.060]  por medio de sus autores...\n",
      "[01:55.060 --> 01:58.740]  cuáles son las nuevas fórmulas para combatir este flagelo.\n",
      "[01:58.740 --> 02:06.060]  Tener en cuenta que las autoridades necesitan músculo de inteligencia y músculo de investigación criminal\n",
      "[02:06.060 --> 02:09.660]  con el propósito de fortalecer la lucha contra la corrupción.\n",
      "[02:09.660 --> 02:13.060]  Donde hay corrupción, donde hay delito, hay corrupción.\n",
      "[02:13.060 --> 02:16.060]  La corrupción es el principal combustible de las economías criminales.\n",
      "[02:16.060 --> 02:23.020]  También para desarticular las estructuras criminales, las cabezas, estas mafias que están detrás de este sistema de conducta.\n",
      "[02:23.060 --> 02:25.060]  La corrupción es un gran problema para el país.\n",
      "[02:25.060 --> 02:27.060]  La corrupción es un gran problema para la economía criminal.\n",
      "[02:27.060 --> 02:29.060]  Tercero, para perseguir el dinero que es la principal motivación de estas economías criminales.\n",
      "[02:29.060 --> 02:35.060]  Y cuarto, en el propósito de quitarles los bienes con procesos de extinción de dominio que les permite...\n",
      "[02:35.060 --> 02:39.060]  Nico Medefrias, AN Panamá, Noticias de Panamá para el Mundo.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "import numpy as np\n",
    "from carlos_tools_audio import OpenAI_transcribe, local_whisper_transcribe, local_faster_whisper_transcribe\n",
    "from carlos_tools_misc import clear_GPU_cache\n",
    "import tempfile\n",
    "import soundfile as sf\n",
    "\n",
    "# Import your local whisper and faster-whisper models\n",
    "from transformers import pipeline\n",
    "import faster_whisper\n",
    "\n",
    "# Dummy GPT remote transcription function (replace with your actual API call)\n",
    "def gpt_transcribe(path):\n",
    "    response = OpenAI_transcribe(\n",
    "        path,\n",
    "        model=\"whisper-1\",\n",
    "        response_format=\"text\"\n",
    "    )\n",
    "    text=response[\"text\"]\n",
    "    duration=response[\"inference_time\"]\n",
    "    return text, duration\n",
    "\n",
    "# Local Whisper\n",
    "def whisper_transcribe(path):\n",
    "    clear_GPU_cache()\n",
    "    response = local_whisper_transcribe(\n",
    "        path,\n",
    "        model_size=\"large-v3\",\n",
    "    )\n",
    "    text= response[\"text\"]\n",
    "    duration= response[\"inference_time\"]\n",
    "    return text, duration\n",
    "\n",
    "# Faster Whisper\n",
    "def faster_whisper_transcribe(path):\n",
    "    clear_GPU_cache()\n",
    "    response = local_faster_whisper_transcribe(\n",
    "        path,\n",
    "        model_size=\"distil-large-v3\",\n",
    "    )\n",
    "    text = response[\"text\"]\n",
    "    duration = response[\"inference_time\"]\n",
    "    return text, duration\n",
    "\n",
    "def compare_transcriptions(path):\n",
    "    gpt_text, gpt_time = gpt_transcribe(path)\n",
    "    faster_text, faster_time = faster_whisper_transcribe(path)\n",
    "    whisper_text, whisper_time = whisper_transcribe(path)\n",
    "    table = [\n",
    "        # [\"Model\", \"Transcription\", \"Duration (s)\"],\n",
    "        [\"GPT (remote)\", gpt_text, round(gpt_time, 2)],\n",
    "        [\"Whisper (local)\", whisper_text, round(whisper_time, 2)],\n",
    "        [\"Faster Whisper (local)\", faster_text, round(faster_time, 2)],\n",
    "    ]\n",
    "    return table\n",
    "\n",
    "with gr.Blocks() as demo:   # may tray themes e.g. theme=gr.themes.Soft()\n",
    "    with gr.Sidebar(position=\"left\", width=200, visible=True):\n",
    "        gr.Markdown(\"# Carlos' tests\")\n",
    "        gr.Markdown(\"## Audio\")\n",
    "        gr.Markdown(\"### [Speech-To-Text](https://huggingface.co/openai/whisper-large-v3)\")\n",
    "\n",
    "    gr.Markdown(\"\"\"\n",
    "            # Speech-To-Text tasks\n",
    "            \"\"\")\n",
    "    with gr.Tab(\"Transcribe\"):\n",
    "            with gr.Row():\n",
    "                gr.Markdown(\"\"\"\n",
    "                            # TRANSCRIPTION\n",
    "                            ## compare transcriptions from different models\n",
    "                            ### Models used: \n",
    "                            \"\"\")\n",
    "            with gr.Row():\n",
    "                gr.Markdown(\"\"\"\n",
    "                            ### [Whisper](https://huggingface.co/openai/whisper-large-v3)\n",
    "\n",
    "                            Whisper is  essentially a language model grounded in audio — an audio-conditional GPT. It was trained by OpenAI on a large and diverse dataset of multilingual audio, enabling it to perform automatic speech recognition (ASR) and translation tasks across many languages.\n",
    "\n",
    "                            Whisper is trained in a similar fashion to the original GPT, using self-supervised learning with a next-token prediction objective. However, while GPT is trained solely on text, Whisper is trained on paired audio and text, where the model learns to generate transcriptions (or translations) token by token from audio inputs. During training, the encoder processes the audio into latent representations, and the decoder learns to predict the next text token given the previous tokens and the audio context. Unlike GPT, which relies purely on textual continuity, Whisper must also learn alignment between speech and language, making it a multimodal model trained end-to-end on large-scale audio-text datasets.\n",
    "                            \"\"\")\n",
    "                gr.Markdown(\"\"\"\n",
    "                            ### [Faster Whisper](https://huggingface.co/Systran/faster-whisper-large-v3)\n",
    "\n",
    "                            Faster-Whisper is a high-performance, inference-optimized implementation of OpenAI's Whisper model — effectively a language model grounded in audio, engineered for fast, resource-efficient deployment.\n",
    "\n",
    "                            While it retains the same underlying architecture and training paradigm as Whisper — a multimodal encoder-decoder transformer trained via self-supervised next-token prediction on paired audio-text data — Faster-Whisper focuses entirely on inference-time efficiency. It uses CTranslate2, a highly optimized inference engine for transformer models, to significantly accelerate transcription and translation while reducing memory usage.\n",
    "\n",
    "                            Like Whisper, Faster-Whisper takes in raw audio, encodes it into latent representations via the audio encoder, and then decodes text token by token, conditioned on both the encoded audio and previously generated tokens. However, all training is inherited directly from the original Whisper checkpoints — Faster-Whisper is not retrained, but instead recompiled and optimized for speed and portability (e.g., on CPU, GPU, or ARM devices).\n",
    "\n",
    "                            As a result, Faster-Whisper makes Whisper’s powerful multilingual speech recognition capabilities more accessible in production environments, edge devices, and real-time applications where latency and efficiency are critical.\n",
    "                            \"\"\")\n",
    "\n",
    "            audio_input = gr.Audio(sources=\"upload\", type= \"filepath\", label=\"Upload Audio\")\n",
    "            output_table = gr.Dataframe(\n",
    "                headers=[\"Model\", \"Translation\", \"Duration (s)\"],\n",
    "                datatype=[\"str\", \"str\", \"number\"],  # Ensure \"Translation\" is string\n",
    "                row_count=5,  # Adjust as needed for visible rows\n",
    "                interactive=False\n",
    ")\n",
    "            transcribe_btn = gr.Button(\"Transcribe with All Models\")\n",
    "            transcribe_btn.click(compare_transcriptions, inputs=audio_input, outputs=output_table)\n",
    "    with gr.Tab(\"Translate\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "                        # Speech-To-Text tasks: TRANSLATION\n",
    "                        ## compare translations from different models\n",
    "                        ### Models used: \n",
    "                        ### [Whisper](https://huggingface.co/openai/whisper-large-v3)\n",
    "\n",
    "                        Whisper is  essentially a language model grounded in audio — an audio-conditional GPT. It was trained by OpenAI on a large and diverse dataset of multilingual audio, enabling it to perform automatic speech recognition (ASR) and translation tasks across many languages.\n",
    "\n",
    "                        Whisper is trained in a similar fashion to the original GPT, using self-supervised learning with a next-token prediction objective. However, while GPT is trained solely on text, Whisper is trained on paired audio and text, where the model learns to generate transcriptions (or translations) token by token from audio inputs. During training, the encoder processes the audio into latent representations, and the decoder learns to predict the next text token given the previous tokens and the audio context. Unlike GPT, which relies purely on textual continuity, Whisper must also learn alignment between speech and language, making it a multimodal model trained end-to-end on large-scale audio-text datasets.\n",
    "\n",
    "                        ### [Faster Whisper](https://huggingface.co/Systran/faster-whisper-large-v3)\n",
    "\n",
    "                        Faster-Whisper is a high-performance, inference-optimized implementation of OpenAI's Whisper model — effectively a language model grounded in audio, engineered for fast, resource-efficient deployment.\n",
    "\n",
    "                        While it retains the same underlying architecture and training paradigm as Whisper — a multimodal encoder-decoder transformer trained via self-supervised next-token prediction on paired audio-text data — Faster-Whisper focuses entirely on inference-time efficiency. It uses CTranslate2, a highly optimized inference engine for transformer models, to significantly accelerate transcription and translation while reducing memory usage.\n",
    "\n",
    "                        Like Whisper, Faster-Whisper takes in raw audio, encodes it into latent representations via the audio encoder, and then decodes text token by token, conditioned on both the encoded audio and previously generated tokens. However, all training is inherited directly from the original Whisper checkpoints — Faster-Whisper is not retrained, but instead recompiled and optimized for speed and portability (e.g., on CPU, GPU, or ARM devices).\n",
    "\n",
    "                        As a result, Faster-Whisper makes Whisper’s powerful multilingual speech recognition capabilities more accessible in production environments, edge devices, and real-time applications where latency and efficiency are critical.\n",
    "                        \"\"\")\n",
    "            audio_input = gr.Audio(sources=\"upload\", type= \"filepath\", label=\"Upload Audio\")\n",
    "            # output_table = gr.Dataframe(headers=[\"Model\", \"Translation\", \"Duration (s)\"], interactive=False)\n",
    "            output_table = gr.Dataframe(\n",
    "                headers=[\"Model\", \"Translation\", \"Duration (s)\"],\n",
    "                datatype=[\"str\", \"str\", \"number\"],  # Ensure \"Translation\" is string\n",
    "                row_count=5,  # Adjust as needed for visible rows\n",
    "                interactive=False\n",
    ")\n",
    "            transcribe_btn = gr.Button(\"Translate with All Models\")\n",
    "            transcribe_btn.click(compare_transcriptions, inputs=audio_input, outputs=output_table)\n",
    "    with gr.Tab(\"Detect language\"):\n",
    "            gr.Markdown(\"\"\"\n",
    "                        # Speech-To-Text tasks: LANGUAGE DETECTION\n",
    "                        ## compare language detection from different models\n",
    "                        ### Models used: \n",
    "                        ### [Whisper](https://huggingface.co/openai/whisper-large-v3)\n",
    "\n",
    "                        Whisper is  essentially a language model grounded in audio — an audio-conditional GPT. It was trained by OpenAI on a large and diverse dataset of multilingual audio, enabling it to perform automatic speech recognition (ASR) and translation tasks across many languages.\n",
    "\n",
    "                        Whisper is trained in a similar fashion to the original GPT, using self-supervised learning with a next-token prediction objective. However, while GPT is trained solely on text, Whisper is trained on paired audio and text, where the model learns to generate transcriptions (or translations) token by token from audio inputs. During training, the encoder processes the audio into latent representations, and the decoder learns to predict the next text token given the previous tokens and the audio context. Unlike GPT, which relies purely on textual continuity, Whisper must also learn alignment between speech and language, making it a multimodal model trained end-to-end on large-scale audio-text datasets.\n",
    "\n",
    "                        ### [Faster Whisper](https://huggingface.co/Systran/faster-whisper-large-v3)\n",
    "\n",
    "                        Faster-Whisper is a high-performance, inference-optimized implementation of OpenAI's Whisper model — effectively a language model grounded in audio, engineered for fast, resource-efficient deployment.\n",
    "\n",
    "                        While it retains the same underlying architecture and training paradigm as Whisper — a multimodal encoder-decoder transformer trained via self-supervised next-token prediction on paired audio-text data — Faster-Whisper focuses entirely on inference-time efficiency. It uses CTranslate2, a highly optimized inference engine for transformer models, to significantly accelerate transcription and translation while reducing memory usage.\n",
    "\n",
    "                        Like Whisper, Faster-Whisper takes in raw audio, encodes it into latent representations via the audio encoder, and then decodes text token by token, conditioned on both the encoded audio and previously generated tokens. However, all training is inherited directly from the original Whisper checkpoints — Faster-Whisper is not retrained, but instead recompiled and optimized for speed and portability (e.g., on CPU, GPU, or ARM devices).\n",
    "\n",
    "                        As a result, Faster-Whisper makes Whisper’s powerful multilingual speech recognition capabilities more accessible in production environments, edge devices, and real-time applications where latency and efficiency are critical.\n",
    "                        \"\"\")\n",
    "            audio_input = gr.Audio(sources=\"upload\", type= \"filepath\", label=\"Upload Audio\")\n",
    "            output_table = gr.Dataframe(headers=[\"Model\", \"Language\", \"Duration (s)\"], interactive=False)\n",
    "            transcribe_btn = gr.Button(\"Detect Language with All Models\")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b50800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whisper in HF\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-large-v3-turbo\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0\n",
    ")\n",
    "pipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78dfc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "\n",
    "def generate_pet_name(animal_type, personality):\n",
    "    cute_prefixes = [\"Fluffy\", \"Ziggy\", \"Bubbles\", \"Pickle\", \"Waffle\", \"Mochi\", \"Cookie\", \"Pepper\"]\n",
    "    animal_suffixes = {\n",
    "        \"Cat\": [\"Whiskers\", \"Paws\", \"Mittens\", \"Purrington\"],\n",
    "        \"Dog\": [\"Woofles\", \"Barkington\", \"Waggins\", \"Pawsome\"],\n",
    "        \"Bird\": [\"Feathers\", \"Wings\", \"Chirpy\", \"Tweets\"],\n",
    "        \"Rabbit\": [\"Hops\", \"Cottontail\", \"Bouncy\", \"Fluff\"]\n",
    "    }\n",
    "\n",
    "    prefix = random.choice(cute_prefixes)\n",
    "    suffix = random.choice(animal_suffixes[animal_type])\n",
    "\n",
    "    if personality == \"Silly\":\n",
    "        prefix = random.choice([\"Sir\", \"Lady\", \"Captain\", \"Professor\"]) + \" \" + prefix\n",
    "    elif personality == \"Royal\":\n",
    "        suffix += \" the \" + random.choice([\"Great\", \"Magnificent\", \"Wise\", \"Brave\"])\n",
    "\n",
    "    return f\"{prefix} {suffix}\"\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    with gr.Sidebar(position=\"left\"):\n",
    "        gr.Markdown(\"# 🐾 Pet Name Generator\")\n",
    "        gr.Markdown(\"Use the options below to generate a unique pet name!\")\n",
    "\n",
    "        animal_type = gr.Dropdown(\n",
    "            choices=[\"Cat\", \"Dog\", \"Bird\", \"Rabbit\"],\n",
    "            label=\"Choose your pet type\",\n",
    "            value=\"Cat\"\n",
    "        )\n",
    "        personality = gr.Radio(\n",
    "            choices=[\"Normal\", \"Silly\", \"Royal\"],\n",
    "            label=\"Personality type\",\n",
    "            value=\"Normal\"\n",
    "        )\n",
    "\n",
    "    name_output = gr.Textbox(label=\"Your pet's fancy name:\", lines=2)\n",
    "    generate_btn = gr.Button(\"Generate Name! 🎲\", variant=\"primary\")\n",
    "    generate_btn.click(\n",
    "        fn=generate_pet_name,\n",
    "        inputs=[animal_type, personality],\n",
    "        outputs=name_output\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be19c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_textbox = gr.Textbox()\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Examples([\"hello\", \"bonjour\", \"merhaba\"], input_textbox)\n",
    "    input_textbox.render()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac33cfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared.\n"
     ]
    }
   ],
   "source": [
    "from carlos_tools_misc import clear_GPU_cache\n",
    "clear_GPU_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
