{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68322dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Carlos\\Documents\\Code\\AI_media\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7883\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7883/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#beyond the interface class, working with blocks\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    name = gr.Textbox(label=\"Name\")\n",
    "    output = gr.Textbox(label=\"Output Box\")\n",
    "    greet_btn = gr.Button(\"Greet\")\n",
    "    greet_btn.click(fn=greet, inputs=name, outputs=output, api_name=\"greet\")\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8856d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7884\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7884/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# event listeners using decorators\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    name = gr.Textbox(label=\"Name\")\n",
    "    output = gr.Textbox(label=\"Output Box\")\n",
    "    greet_btn = gr.Button(\"Greet\")\n",
    "\n",
    "    @greet_btn.click(inputs=name, outputs=output)\n",
    "    def greet(name):\n",
    "        return \"Hello \" + name + \"!\"\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0375381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7885\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7885/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def welcome(name):\n",
    "    return f\"Welcome to Gradio, {name}!\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    # Hello World!\n",
    "    Start typing below to see the output.\n",
    "    \"\"\")\n",
    "    inp = gr.Textbox(placeholder=\"What is your name?\")\n",
    "    out = gr.Textbox()\n",
    "    inp.change(welcome, inp, out)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11fcbb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7886\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7886/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def increase(num):\n",
    "    return num + 1\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    a = gr.Number(label=\"a\")\n",
    "    b = gr.Number(label=\"b\")\n",
    "    atob = gr.Button(\"a > b\")\n",
    "    btoa = gr.Button(\"b > a\")\n",
    "    atob.click(increase, a, b)\n",
    "    btoa.click(increase, b, a)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5584d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7888\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7888/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "asr = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")\n",
    "classifier = pipeline(\"text-classification\")\n",
    "\n",
    "def speech_to_text(speech):\n",
    "    text = asr(speech)[\"text\"]  \n",
    "    return text\n",
    "\n",
    "def text_to_sentiment(text):\n",
    "    return classifier(text)[0][\"label\"]  \n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    audio_file = gr.Audio(type=\"filepath\")\n",
    "    text = gr.Textbox()\n",
    "    label = gr.Label()\n",
    "\n",
    "    b1 = gr.Button(\"Recognize Speech\")\n",
    "    b2 = gr.Button(\"Classify Sentiment\")\n",
    "\n",
    "    b1.click(speech_to_text, inputs=audio_file, outputs=text)\n",
    "    b2.click(text_to_sentiment, inputs=text, outputs=label)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1269b5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7889\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7889/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multiple inputs components\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    a = gr.Number(label=\"a\")\n",
    "    b = gr.Number(label=\"b\")\n",
    "    with gr.Row():\n",
    "        add_btn = gr.Button(\"Add\")\n",
    "        sub_btn = gr.Button(\"Subtract\")\n",
    "    c = gr.Number(label=\"sum\")\n",
    "\n",
    "    def add(num1, num2):\n",
    "        return num1 + num2\n",
    "    add_btn.click(add, inputs=[a, b], outputs=c)\n",
    "\n",
    "    def sub(data):\n",
    "        return data[a] - data[b]\n",
    "    sub_btn.click(sub, inputs={a, b}, outputs=c)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80673ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared.\n"
     ]
    }
   ],
   "source": [
    "clear_GPU_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a04e9aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Carlos\\Documents\\Code\\AI_media\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.12.2)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "* Running on local URL:  http://127.0.0.1:7883\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7883/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared.\n",
      "Running faster whisper model locally. \n",
      "file_path='C:\\\\Users\\\\cfune\\\\AppData\\\\Local\\\\Temp\\\\gradio\\\\e1442c6f73c6328e767a89521b02f7ef68af14e6bac3e4168b9202d9e244855d\\\\test.wav'\n",
      " model_size='distil-large-v3'\n",
      " device='cuda'\n",
      " compute_type='float16'\n",
      " language=None\n",
      " prompt=None\n",
      "\n",
      "Detected language en with probability 0.98681640625\n",
      "GPU cache cleared.\n",
      "Running whisper model locally. \n",
      "file_path='C:\\\\Users\\\\cfune\\\\AppData\\\\Local\\\\Temp\\\\gradio\\\\e1442c6f73c6328e767a89521b02f7ef68af14e6bac3e4168b9202d9e244855d\\\\test.wav'\n",
      " model_size='large-v3'\n",
      " device='cuda'\n",
      " verbose=True\n",
      " prompt=None\n",
      " language=None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Carlos\\Documents\\Code\\AI_media\\env\\Lib\\site-packages\\torch\\cuda\\__init__.py:209: UserWarning: \n",
      "NVIDIA GeForce RTX 5060 Ti with CUDA capability sm_120 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 sm_80 sm_86 sm_90 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 5060 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting language using up to the first 30 seconds. Use `--language` to specify the language\n",
      "Detected language: English\n",
      "[00:00.000 --> 00:08.320]  This is a test. 1 2 3 4 5 6 7 8 1 2 3 1 2 3\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "import numpy as np\n",
    "from carlos_tools_audio import OpenAI_transcribe, local_whisper_transcribe, local_faster_whisper_transcribe\n",
    "from carlos_tools_misc import clear_GPU_cache\n",
    "import tempfile\n",
    "import soundfile as sf\n",
    "\n",
    "# Import your local whisper and faster-whisper models\n",
    "from transformers import pipeline\n",
    "import faster_whisper\n",
    "\n",
    "# Dummy GPT remote transcription function (replace with your actual API call)\n",
    "def gpt_transcribe(path):\n",
    "    response = OpenAI_transcribe(\n",
    "        path,\n",
    "        model=\"whisper-1\",\n",
    "        response_format=\"text\"\n",
    "    )\n",
    "    text=response[\"text\"]\n",
    "    duration=response[\"inference_time\"]\n",
    "    return text, duration\n",
    "\n",
    "# Local Whisper\n",
    "def whisper_transcribe(path):\n",
    "    clear_GPU_cache()\n",
    "    response = local_whisper_transcribe(\n",
    "        path,\n",
    "        model_size=\"large-v3\",\n",
    "    )\n",
    "    text= response[\"text\"]\n",
    "    duration= response[\"inference_time\"]\n",
    "    return text, duration\n",
    "\n",
    "# Faster Whisper\n",
    "def faster_whisper_transcribe(path):\n",
    "    clear_GPU_cache()\n",
    "    response = local_faster_whisper_transcribe(\n",
    "        path,\n",
    "        model_size=\"distil-large-v3\",\n",
    "    )\n",
    "    text = response[\"text\"]\n",
    "    duration = response[\"inference_time\"]\n",
    "    return text, duration\n",
    "\n",
    "def compare_transcriptions(path):\n",
    "    gpt_text, gpt_time = gpt_transcribe(path)\n",
    "    faster_text, faster_time = faster_whisper_transcribe(path)\n",
    "    whisper_text, whisper_time = whisper_transcribe(path)\n",
    "    table = [\n",
    "        [\"Model\", \"Transcription\", \"Duration (s)\"],\n",
    "        [\"GPT (remote)\", gpt_text, round(gpt_time, 2)],\n",
    "        [\"Whisper (local)\", whisper_text, round(whisper_time, 2)],\n",
    "        [\"Faster Whisper (local)\", faster_text, round(faster_time, 2)],\n",
    "    ]\n",
    "    return table\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Audio Transcription Comparison\")\n",
    "    audio_input = gr.Audio(sources=\"upload\", type= \"filepath\", label=\"Upload Audio\")\n",
    "    output_table = gr.Dataframe(headers=[\"Model\", \"Transcription\", \"Duration (s)\"], interactive=False)\n",
    "    transcribe_btn = gr.Button(\"Transcribe with All Models\")\n",
    "    transcribe_btn.click(compare_transcriptions, inputs=audio_input, outputs=output_table)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b50800",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\Carlos\\Documents\\Code\\AI_media\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\cfune\\.cache\\huggingface\\hub\\models--openai--whisper-large-v3-turbo. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cuda:0\n",
      "d:\\Users\\Carlos\\Documents\\Code\\AI_media\\env\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "d:\\Users\\Carlos\\Documents\\Code\\AI_media\\env\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whisper in HF\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(\n",
    "    task=\"automatic-speech-recognition\",\n",
    "    model=\"openai/whisper-large-v3-turbo\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device=0\n",
    ")\n",
    "pipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
